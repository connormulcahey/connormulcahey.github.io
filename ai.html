<html>
<head>
	<meta charset="UTF-8">
	<title>AI: Should We Be Worried?</title>
	<link rel="stylesheet" href="style.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="shortcut icon" href="/favicon.png">
</head>
<body>
	<h1>AI: Should We Be Worried?</h1>
	<h2>Connor Mulcahey</h2>
	<h3 style="text-align: center;"><strong>4/25/2017</strong></h3>

	<br>

	<p>So, I'm a newcomer to machine learning and these days I learn best by targeted and self-directed creativity and curiostiy over more traditional methods. In the spirit of <em><a target="_blank" href="http://waitbutwhy.com/">Wait But Why</a></em> which got me interested in this topic in the first place, I'm trying something new with my learning where I document my curiosity via creativity and publish it on the web for others who had the same questions I did.</p>

	<p>Here goes nothing.</p>

	<hr>

	<h3>Background</h3>

	<p>The fundamental curiosities I have with regards to AI are not with bayesian classifiers, better content recommendation systems, or chasing <a target="_blank" href="https://en.wikipedia.org/wiki/Deep_learning">concepts with mysterious sounding names</a> but rather with the technical underpinnings, timeline, and safety discussions surrounding the inception of <a target="_blank" href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial general intelligence</a> as well as the risks posed by the <a target="_blank" href="">portability</a> of other powerful AI algorithms. Initially I became interested in this after reading the somewhat well known article on <a target="_blank" href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">WaitButWhy</a> which, while it has received warranted <a target="_blank" href="http://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/">criticism</a>, has successfully introduced <a target="_blank" href="https://twitter.com/elonmusk/status/702534707464896512?lang=en">a lot more people</a> to the topic (hopefully for the better).</p>

	<p>The article prompted me to read <em><a target="_blank" href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Superintelligence: Paths, Dangers, Strategies</a></em> by <a target="_blank" href="http://www.nickbostrom.com/">Nick Bostrom</a> along with some of his <a target="_blank" href="http://www.nickbostrom.com/fable/dragon.html">other</a> <a target="_blank" href="http://www.nickbostrom.com/existential/risks.html">existential</a> <a target="_blank" href="http://www.simulation-argument.com/simulation.html">crisis</a> <a target="_blank" href="http://www.anthropic-principle.com/?q=anthropic_bias">catalysts</a>.</p>

	<p>Since then I've tried to keep up with the trajectory of the field because, as an intelligent agent myself, I rather instinctively desire to forcast the future states my environment will be in. However, up until this point, I've been merely aggregating the opinions of others, such as this <a target="_blank" href="http://www.cnbc.com/2014/11/17/elon-musks-deleted-message-five-years-until-dangerous-ai.html">deleted comment</a>:</p>

	<img src="elon-deleted.jpg" alt="">

	<p>And this complementary <a target="_blank" href="https://www.theguardian.com/technology/2016/feb/16/demis-hassabis-artificial-intelligence-deepmind-alphago">expert opinion</a>: </p>

	<blockquote>
		“Elon is one of the smartest people out there, and amazing to talk to,” Hassabis responds, neutrally. “And I actually think it’s pretty cool people like him are getting so into AI because it just shows what a big deal it is.” He remains diplomatic, but it evidently irritates him that scientists from other areas feel at liberty to pronounce publicly on AI: you don’t hear him pontificating about particle physics, after all.
		<br>
		“In general, I’ve found that people who don’t actually work on AI don’t fully understand it. They often haven’t talked to many AI experts, so their thought experiments are running away with themselves because they’re based on what I think are going to turn out to be incorrect assumptions.”

		<br><br>

		&mdash; <a target="_blank" href="https://en.wikipedia.org/wiki/Demis_Hassabis">Demis Hassabis,</a> founder, <a target="_blank" href="https://www.youtube.com/watch?v=rbsqaJwpu6A">DeepMind</a></a>
	</blockquote>

	<p>Demis describes DeepMind as a sort of Apollo program for artificial general intelligence with over 100 talented PhDs gathered to conduct the research &mdash; notably without the labyrinthine bureaucracies and chores present in academia. In 2015 Demis <a href="https://youtu.be/rbsqaJwpu6A?t=6m5s">stated</a> that DeepMind's mission was thus:</p>

	<img src="solve.png" alt="">

	<p>However, since then, it's changed to the more indefinite:</p>

	<img src="solve2.png" alt="">

	<p>So this leaves me wondering why it was changed. It could be that we are so very far away that this mission would be like Kennedy proclaiming that we go to <em>Mars</em> in the 60's. Or it could be that Google/DeepMind does not want the rest of the world/company feeling that our work is realtively ephemeral like so:</p>

	<blockquote>
		Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.

		<br><br>

		&mdash; <strong>Nick Bostrom</strong> in an <a href="http://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom">interview</a> with <strong>The New Yorker</strong>
	</blockquote>

	<p>For now I think it's probably some of both.</p>

	<p>Unlike Bostrom and Hassabis, there is an archetypal academic viewpoint that's devoid of anxiety such as that of <a target="_blank" href="http://www.andrewng.org/">Andrew Ng</a>, one of the most well known researchers in the field:

	<iframe  width="560" height="315" src="https://www.youtube.com/embed/21EiKfQYZXc?start=2265" frameborder="0" allowfullscreen="allowfullscreen" id="fitvid593146"></iframe>

	<p>Immediately before this description of the <em><strong>non-virtuous circle of AI</strong></em>, Andrew articultes the <strong><em>virtuous circle of AI</em></strong> which precisely descibes the sector of AI that I'm <strong>not</strong> interested in. While I concede that there does seem to be some of deliberate "non-virtuous" activity going on, it seems foolish to completely disregard what is certainly possible long term regardless of the simplicity of our current algorithms.</p>

	<p>Even though these experts say that AGI is still decades away, it's still not a matter of <strong>if</strong> but <strong>when</strong> and <strong>how</strong>. Futhermore, sentience is not essential for AI agents to be dangerous. For example, by simply making algorithms, computational resources, and perhaps robotic components accessible, a bad actor could construct an autonimous terror agent. We can hardly control who gets on a plane or can obtain a gun, let alone who can write computer programs and access the internet.</p>

	<p>It reminds me of this quote from <a href="https://www.amazon.com/Signal-Noise-Many-Predictions-Fail/dp/159420411X"><em>The Signal And The Noise</em></a> by Nate Silver:</p>

	<blockquote>
		“Human beings have an extraordinary capacity to ignore risks that threaten their livelihood, as though this will make them go away.”
	</blockquote>

	<p>Then, with AGI, it doesn't really matter how far it seems we are away from general intelligence based on how little we understand the brain and how simple our algorithms seem relative to its complexity. Fundamentally it's just a reinforcement learning agent in a complex environment. We're going to get there eventually unless technological progress is interrupted.</p>

	<p>So my question fundamentally is, precisely where are we, and how far do we have to go? I want to have some basis for forming my own opinion from the science alone instead of the high level opinions of experts.</p>

	<p>With my current limited exposure to the field it seems to me that the best place to start in answering this question is by fully understanding DeepMind's <em><a target="_blank" href="https://arxiv.org/pdf/1312.5602v1.pdf">Playing Atari with Deep Reinforcement Learning</a></em> paper.</p>

	<p>So, here goes.</p>

	<h3>Understanding <em>Playing Atari with Deep Reinforcement Learning</em></h3>

	<p>My fundamental question I had going into this was the following: how generic is this algorithm? Could it easily be scaled to work with other kinds of tensors &mdash; textual, etc.</p>

	<p>Here is the  published on February 26th 2015. It was featured on the cover of <a target="_blank" href="http://www.nature.com/">Nature:</a></p>

	<img src="https://lh6.googleusercontent.com/-Je-VCqhBoCE/VO5DRi39teI/AAAAAAAAC_U/5wSiWJ4F1Gg/w506-h750/DeepMindNatureCover.jpg" alt="">

	<p>Here is a video demonstration:</p>

	<iframe width="560" height="315" src="https://www.youtube.com/embed/V1eYniJ0Rnk" frameborder="0" allowfullscreen></iframe>

	<p>And here is the <a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner">code</a>.</p>

	

</body>
</html>
